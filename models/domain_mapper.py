"""
Train state mappings with dht models.
"""

import os
import sys
from pathlib import Path

import numpy as np
import torch
from torch.utils.data import DataLoader, TensorDataset
from torch.nn import MSELoss
import pytorch_lightning as pl
from pytorch_lightning.trainer.supporters import CombinedLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau

sys.path.append(str(Path(__file__).resolve().parents[1]))

from utils.nn import NeuralNetwork, KinematicChainLoss
from utils.dht import get_dht_model

from config import data_folder


class LitDomainMapper(pl.LightningModule):
    def __init__(self,
                 data_file_A, data_file_B,
                 transition_model_network_width=32, transition_model_network_depth=1, transition_model_dropout=0.,
                 transition_model_lr=3e-4,
                 state_mapper_network_width=32, state_mapper_network_depth=1, state_mapper_dropout=0.,
                 state_mapper_lr=3e-4,
                 action_mapper_network_width=32, action_mapper_network_depth=1, action_mapper_dropout=0.,
                 action_mapper_lr=3e-4,
                 weight_matrix_exponent=np.inf,
                 batch_size=32,
                 # growth_fn=lambda epoch: 1.,
                 num_workers=1,
                 components=["s", "a", "t"],
                 **kwargs
                 ):
        super(LitDomainMapper, self).__init__()
        self.save_hyperparameters()

        self.dht_model_A, self.transition_model_A, self.state_mapper_AB, self.action_mapper_AB = \
            self.get_models(data_file_A, data_file_B,
                            transition_model_network_width, transition_model_network_depth, transition_model_dropout,
                            state_mapper_network_width, state_mapper_network_depth, state_mapper_dropout,
                            action_mapper_network_width, action_mapper_network_depth, action_mapper_dropout,
                            )
        self.dht_model_B, self.transition_model_B, self.state_mapper_BA, self.action_mapper_BA = \
            self.get_models(data_file_B, data_file_A,
                            transition_model_network_width, transition_model_network_depth, transition_model_dropout,
                            state_mapper_network_width, state_mapper_network_depth, state_mapper_dropout,
                            action_mapper_network_width, action_mapper_network_depth, action_mapper_dropout,
                            )

        self.loss_fn_transition_model, self.loss_fn_kinematics_AB, self.loss_fn_kinematics_BA = self.get_loss_functions()

        # self.growth_fn = growth_fn

        self.relevant_optimizers = set()

        if "t" in components:
            self.relevant_optimizers.update([0, 1])
        if "s" in components:
            self.relevant_optimizers.update([2, 3])
        if "a" in components:
            self.relevant_optimizers.update([4, 5])

        # manual optimization in training_step
        self.automatic_optimization = False

    """
        Maps states and actions A -> B
        Required by super class LightningModule
        
        Args:
            state_A
            action_A
        Returns:
            Mapped state and action
    """

    def forward(self, state_A, action_A):
        return self.state_mapper_AB(state_A), self.action_mapper_AB(action_A)

    """
        Helper function to generate all models required to learn mapping X -> Y
        
        Args:
            data_file_X, data_file_Y: Data files for both domains generated by 01_generate_data.py
            {transition_model, state_mapper, action_mapper}_{network_width, network_depth, dropout}: 
                Hyperparameters for neural networks
                
        Returns:
            Pytorch models according to specified hyperparameters
    """

    def get_models(self, data_file_X, data_file_Y,
                   transition_model_network_width, transition_model_network_depth, transition_model_dropout,
                   state_mapper_network_width, state_mapper_network_depth, state_mapper_dropout,
                   action_mapper_network_width, action_mapper_network_depth, action_mapper_dropout,
                   ):
        data_path_X = os.path.join(data_folder, data_file_X)
        data_X = torch.load(data_path_X)

        data_path_Y = os.path.join(data_folder, data_file_Y)
        data_Y = torch.load(data_path_Y)

        dht_model_X = get_dht_model(data_X["dht_params"], data_X["joint_limits"])

        transition_model_X = self.create_network(
            in_dim=data_X["states_train"].shape[1] + data_X["actions_train"].shape[1],
            out_dim=data_X["states_train"].shape[1],
            network_width=transition_model_network_width,
            network_depth=transition_model_network_depth,
            dropout=transition_model_dropout,
            out_activation='tanh'
        )

        state_mapper_XY = self.create_network(
            in_dim=data_X["states_train"].shape[1],
            out_dim=data_Y["states_train"].shape[1],
            network_width=state_mapper_network_width,
            network_depth=state_mapper_network_depth,
            dropout=state_mapper_dropout,
            out_activation='tanh'
        )

        action_mapper_XY = self.create_network(
            in_dim=data_X["actions_train"].shape[1],
            out_dim=data_Y["actions_train"].shape[1],
            network_width=action_mapper_network_width,
            network_depth=action_mapper_network_depth,
            dropout=action_mapper_dropout,
            out_activation='tanh'
        )

        return dht_model_X, transition_model_X, state_mapper_XY, action_mapper_XY

    """
        Helper function to generate all loss functions

        Returns:
            Loss functions
    """

    def get_loss_functions(self):
        data_path_A = os.path.join(data_folder, self.hparams.data_file_A)
        data_A = torch.load(data_path_A)

        data_path_B = os.path.join(data_folder, self.hparams.data_file_B)
        data_B = torch.load(data_path_B)

        loss_fn_transition_model = MSELoss()

        link_positions_A = self.dht_model_A(torch.zeros((1, *data_A["states_train"].shape[1:])))[0, :, :3, -1]
        link_positions_B = self.dht_model_B(torch.zeros((1, *data_B["states_train"].shape[1:])))[0, :, :3, -1]

        weight_matrix_AB_p, weight_matrix_AB_o = self.get_weight_matrices(link_positions_A, link_positions_B,
                                                                          self.hparams.weight_matrix_exponent)

        loss_fn_kinematics_AB = KinematicChainLoss(weight_matrix_AB_p, weight_matrix_AB_o)
        loss_fn_kinematics_BA = KinematicChainLoss(weight_matrix_AB_p.T, weight_matrix_AB_o.T)

        return loss_fn_transition_model, loss_fn_kinematics_AB, loss_fn_kinematics_BA

    """
        Helper function to generate neural network from hyperparameters.
        Activations between layers are ReLU.
        
        Params:
            in_dim: Input dimension of the network
            out_dim: Output dimension of the network
            network_width
            network_depth
            dropout
            out_activation: What activation should be used on the network output
    """

    def create_network(self, in_dim, out_dim, network_width, network_depth, dropout, out_activation=None):
        network_structure = [('linear', network_width), ('relu', None),
                             ('dropout', dropout)] * network_depth
        network_structure.append(('linear', out_dim))

        if out_activation:
            network_structure.append((out_activation, None))

        return NeuralNetwork(in_dim, network_structure)

    """
        Generate weights based on distances between relative positions of robot links
        
        Params:
            link_positions_{X, Y}: Positions of both robots in 3D space.
            weight_matrix_exponent_p: Parameter used to shape the position weight matrix by emphasizing similarity.
                weight = exp(-weight_matrix_exponent_p * distance)
        
        Returns:
            weight_matrix_XY_p: Weight matrix for positions
            weight_matrix_XY_p: Weight matrix for orientations. All zeros except weight which corresponds to the end effectors.
    """

    @staticmethod
    def get_weight_matrices(link_positions_X, link_positions_Y, weight_matrix_exponent_p, norm=True):
        link_positions_X = torch.cat((torch.zeros(1, 3), link_positions_X))
        link_lenghts_X = torch.norm(link_positions_X[1:] - link_positions_X[:-1], p=2, dim=-1)
        link_order_X = link_lenghts_X.cumsum(0)
        link_order_X = link_order_X / link_order_X[-1]

        link_positions_Y = torch.cat((torch.zeros(1, 3), link_positions_Y))
        link_lenghts_Y = torch.norm(link_positions_Y[1:] - link_positions_Y[:-1], p=2, dim=-1)
        link_order_Y = link_lenghts_Y.cumsum(0)
        link_order_Y = link_order_Y / link_order_Y[-1]

        weight_matrix_XY_p = torch.exp(
            -weight_matrix_exponent_p * torch.cdist(link_order_X.unsqueeze(-1), link_order_Y.unsqueeze(-1)))
        weight_matrix_XY_p = torch.nan_to_num(weight_matrix_XY_p, 1.)

        weight_matrix_XY_o = torch.zeros(len(link_positions_X), len(link_positions_Y))
        weight_matrix_XY_o[-1, -1] = 1

        if norm:
            weight_matrix_XY_p /= weight_matrix_XY_p.sum()
            weight_matrix_XY_o /= weight_matrix_XY_o.sum()

        return weight_matrix_XY_p, weight_matrix_XY_p

    def get_train_dataloader(self, data_file):
        data_path = os.path.join(data_folder, data_file)
        data = torch.load(data_path)

        states_train = data["states_train"]
        actions_train = data["actions_train"]
        next_states_train = data["next_states_train"]

        dataloader_train = DataLoader(TensorDataset(states_train, actions_train, next_states_train),
                                      batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,
                                      shuffle=True, pin_memory=True)

        return dataloader_train

    def get_validation_dataloader(self, data_file):
        data_path = os.path.join(data_folder, data_file)
        data = torch.load(data_path)

        states_validation = data["states_test"]
        actions_validation = data["actions_test"]
        next_states_validation = data["next_states_test"]

        dataloader_validation = DataLoader(TensorDataset(states_validation, actions_validation, next_states_validation),
                                           batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers,
                                           pin_memory=True)

        return dataloader_validation

    """
        Generate dataloader used for training.
        Refer to pytorch lightning docs.
    """

    def train_dataloader(self):
        dataloader_train_A = self.get_train_dataloader(self.hparams.data_file_A)
        dataloader_train_B = self.get_train_dataloader(self.hparams.data_file_B)
        return CombinedLoader({"A": dataloader_train_A, "B": dataloader_train_B})

    """
        Generate dataloader used for validation.
        Refer to pytorch lightning docs.
    """

    def val_dataloader(self):
        dataloader_validation_A = self.get_validation_dataloader(self.hparams.data_file_A)
        dataloader_validation_B = self.get_validation_dataloader(self.hparams.data_file_B)

        return CombinedLoader({"A": dataloader_validation_A, "B": dataloader_validation_B})

    """
        Determine loss of transition model on batch.
    """

    def loss_transition_model(self, batch, transition_model, loss_fn):
        states, actions, next_states = batch
        states_actions_X = torch.concat((states, actions), axis=-1)
        next_states_X_predicted = transition_model(states_actions_X)
        loss_transition_model = loss_fn(next_states_X_predicted, next_states)
        return loss_transition_model

    """
        Determine loss of state mapper on batch.
    """

    def loss_state_mapper(self, batch, state_mapper_XY, dht_model_X, dht_model_Y, loss_fn):
        states_X, _, _ = batch

        states_Y = state_mapper_XY(states_X)

        link_poses_X = dht_model_X(states_X)
        link_poses_Y = dht_model_Y(states_Y)

        # s = self.growth_fn(self.current_epoch)
        # scaling = torch.ones((4, 4), device=link_poses_Y.device)
        # scaling[:3, -1] = s
        # link_poses_Y_scaled = torch.einsum("blxy,xy->blxy", link_poses_Y, scaling)

        loss_state_mapper_XY, _, _ = loss_fn(link_poses_X, link_poses_Y)

        # error_tcp_p = torch.norm(link_poses_X[:, -1, :3, -1] - link_poses_Y[:, -1, :3, -1], p=2, dim=-1).mean()
        # error_tcp_o = torch.norm(link_poses_X[:, -1, :3, -1] - link_poses_Y[:, -1, :3, -1], p=2, dim=-1).mean()

        return loss_state_mapper_XY

    """
        Determine loss of action mapper on batch.
    """

    def loss_action_mapper(self, batch, action_mapper_XY, state_mapper_XY, dht_model_X, dht_model_Y, transition_model_Y,
                           loss_fn):
        states_X, actions_X, next_states_X = batch

        states_Y = state_mapper_XY(states_X)

        actions_Y = action_mapper_XY(actions_X)

        states_actions_Y = torch.concat((states_Y, actions_Y), axis=-1)

        next_states_Y = transition_model_Y(states_actions_Y)

        link_poses_X = dht_model_X(next_states_X)
        link_poses_Y = dht_model_Y(next_states_Y)

        loss_action_mapper_XY, _, _ = loss_fn(link_poses_X, link_poses_Y)

        return loss_action_mapper_XY

    # def training_epoch_end(self, outputs) -> None:
    #     self.log("growth", self.growth_fn(self.current_epoch))

    """
        Perform training step. Customized behavior to enable accumulation of all losses into one variable.
        Refer to pytorch lightning docs.
    """

    def training_step(self, batch, batch_idx):

        cumulated_loss = 0.

        for optimizer_idx, optimizer in enumerate(self.optimizers()):
            if not optimizer_idx in self.relevant_optimizers:
                continue

            optimizer.zero_grad()
            loss = self.step(batch, batch_idx, optimizer_idx, "train_")
            self.manual_backward(loss)
            optimizer.step()

            cumulated_loss += loss.item()

        self.log("train_loss", cumulated_loss, on_step=False, on_epoch=True)

    """
        Perform validation step. Customized behavior to enable accumulation of all losses into one variable.
        Refer to pytorch lightning docs.
    """

    def validation_step(self, batch, batch_idx):

        cumulated_loss = 0.

        for optimizer_idx in range(len(self.optimizers())):
            if not optimizer_idx in self.relevant_optimizers:
                continue

            loss = self.step(batch, batch_idx, optimizer_idx, "validation_")
            cumulated_loss += loss.item()

        self.log("validation_loss", cumulated_loss, on_step=False, on_epoch=True)

    """
        Perform one step (compute loss) for a model corresponding to an optimizer_idx.
        
        Params:
            batch: data
            batch_idx: not used
            optimizer_idx: Optimizer (and respective model) to evaluate
            log_prefix: used to log training and validation losses under different names
        
        Returns:
            loss
    """

    def step(self, batch, batch_idx, optimizer_idx, log_prefix=""):

        if optimizer_idx == 0:
            loss_transition_model = self.loss_transition_model(batch["A"], self.transition_model_A,
                                                               self.loss_fn_transition_model)
            self.log(log_prefix + 'loss_transition_model_A', loss_transition_model, on_step=False, on_epoch=True)
            return loss_transition_model
        elif optimizer_idx == 1:
            loss_transition_model = self.loss_transition_model(batch["B"], self.transition_model_B,
                                                               self.loss_fn_transition_model)
            self.log(log_prefix + 'loss_transition_model_B', loss_transition_model, on_step=False, on_epoch=True)
            return loss_transition_model
        elif optimizer_idx == 2:
            loss_state_mapper = self.loss_state_mapper(batch["A"], self.state_mapper_AB,
                                                       self.dht_model_A, self.dht_model_B, self.loss_fn_kinematics_AB)
            self.log(log_prefix + 'loss_state_mapper_AB', loss_state_mapper, on_step=False, on_epoch=True)
            return loss_state_mapper
        elif optimizer_idx == 3:
            loss_state_mapper = self.loss_state_mapper(batch["B"], self.state_mapper_BA,
                                                       self.dht_model_B, self.dht_model_A, self.loss_fn_kinematics_BA)
            self.log(log_prefix + 'loss_state_mapper_BA', loss_state_mapper, on_step=False, on_epoch=True)
            return loss_state_mapper
        elif optimizer_idx == 4:
            loss_action_mapper = self.loss_action_mapper(batch["A"], self.action_mapper_AB, self.state_mapper_AB,
                                                         self.dht_model_A, self.dht_model_B, self.transition_model_B,
                                                         self.loss_fn_kinematics_AB)
            self.log(log_prefix + 'loss_action_mapper_AB', loss_action_mapper, on_step=False, on_epoch=True)
            return loss_action_mapper
        elif optimizer_idx == 5:
            loss_action_mapper = self.loss_action_mapper(batch["B"], self.action_mapper_BA, self.state_mapper_BA,
                                                         self.dht_model_B, self.dht_model_A, self.transition_model_A,
                                                         self.loss_fn_kinematics_BA)
            self.log(log_prefix + 'loss_action_mapper_BA', loss_action_mapper, on_step=False, on_epoch=True)
            return loss_action_mapper

    """
        Helper function to generate all optimizers.
        Refer to pytorch lightning docs.
    """

    def configure_optimizers(self):
        optimizer_transition_model_A = torch.optim.Adam(self.transition_model_A.parameters(),
                                                        lr=self.hparams.transition_model_lr)
        optimizer_transition_model_B = torch.optim.Adam(self.transition_model_B.parameters(),
                                                        lr=self.hparams.transition_model_lr)
        optimizer_state_mapper_AB = torch.optim.Adam(self.state_mapper_AB.parameters(),
                                                     lr=self.hparams.state_mapper_lr)
        optimizer_state_mapper_BA = torch.optim.Adam(self.state_mapper_BA.parameters(),
                                                     lr=self.hparams.state_mapper_lr)
        optimizer_action_mapper_AB = torch.optim.Adam(self.action_mapper_AB.parameters(),
                                                      lr=self.hparams.action_mapper_lr)
        optimizer_action_mapper_BA = torch.optim.Adam(self.action_mapper_BA.parameters(),
                                                      lr=self.hparams.action_mapper_lr)

        scheduler_state_mapper_AB = {"scheduler": ReduceLROnPlateau(optimizer_state_mapper_AB),
                                     "monitor": "validation_loss_state_mapper_AB",
                                     "name": "scheduler_optimizer_state_mapper_AB"
                                     }

        scheduler_state_mapper_BA = {"scheduler": ReduceLROnPlateau(optimizer_state_mapper_BA),
                                     "monitor": "validation_loss_state_mapper_BA",
                                     "name": "scheduler_optimizer_state_mapper_BA"
                                     }

        return [optimizer_transition_model_A, optimizer_transition_model_B,
                optimizer_state_mapper_AB, optimizer_state_mapper_BA,
                optimizer_action_mapper_AB, optimizer_action_mapper_BA], \
               [scheduler_state_mapper_AB, scheduler_state_mapper_BA]


if __name__ == '__main__':
    mapper = LitDomainMapper

    domain_mapper = LitDomainMapper(
        data_file_A="panda_10000_1000.pt",
        data_file_B="ur5_10000_1000.pt"
    )

    trainer = pl.Trainer(max_epochs=1)
    trainer.fit(domain_mapper)

    trainer.save_checkpoint("../data/domain_mapper_dummy.chkp")
